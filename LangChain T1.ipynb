{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9ccf430",
   "metadata": {},
   "source": [
    "## Intro to LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b46ef9e",
   "metadata": {},
   "source": [
    "LangChain is a popular framework that allow users to quickly build apps and pipelines around Large Language Models. \n",
    "It can be used to for chatbots, Generative Question-Anwering (GQA), summarization, and much more.\n",
    "\n",
    "The core idea of the library is that we can \"chain\" together different components to create more advanced use-cases around \n",
    "LLMs. Chains may consist of multiple components from several modules:\n",
    "\n",
    "__Prompt templates__: Prompt templates are, well, templates for different types of prompts. Like \"chatbot\" style templates, ELI5 question-answering, etc\n",
    "\n",
    "__LLMs__: Large language models like GPT-3, BLOOM, etc\n",
    "\n",
    "__Agents__: Agents use LLMs to decide what actions should be taken, tools like web search or calculators can be used, and all packaged into logical loop of operations.\n",
    "\n",
    "__Memory__: Short-term memory, long-term memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f72da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/dask/s3fs\n",
      "  Cloning https://github.com/dask/s3fs to c:\\users\\ramshankar\\appdata\\local\\temp\\pip-req-build-46knqtnj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Error [WinError 2] The system cannot find the file specified while executing command git version\n",
      "ERROR: Cannot find command 'git' - do you have 'git' installed and in your PATH?\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade git+https://github.com/dask/s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c33e5b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (0.0.353)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.2 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from langchain) (0.0.7)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.4 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from langchain) (0.1.4)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.70 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from langchain) (0.0.75)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from langchain) (1.10.8)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\ramshankar\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\ramshankar\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<0.2,>=0.1.4->langchain) (3.7.1)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from langchain-core<0.2,>=0.1.4->langchain) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\ramshankar\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.4->langchain) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a4954be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from huggingface_hub) (2023.12.2)\n",
      "Requirement already satisfied: requests in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ramshankar\\appdata\\roaming\\python\\python311\\site-packages (from huggingface_hub) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "##Hugging face model \n",
    "\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25afe2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6509308e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ramshankar\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "\n",
    "# initialize HF LLM\n",
    "flan_t5 = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-xl\",\n",
    "    model_kwargs={\"temperature\":1e-10}\n",
    ")\n",
    "\n",
    "# build prompt template for simple question-answering\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=flan_t5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfa83c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"Which NFL team won the Super Bowl in the 2010 season?\"\n",
    "\n",
    "# print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301b5a10",
   "metadata": {},
   "source": [
    "If we'd like to ask multiple questions we can by passing a list of dictionary objects, where the dictionaries must contain the input variable set in our prompt template (\"question\") that is mapped to the question we'd like to ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40c8fa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = [\n",
    "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
    "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
    "    {'question': \"Who was the 12th person on the moon?\"},\n",
    "    {'question': \"How many eyes does a blade of grass have?\"}\n",
    "]\n",
    "# res = llm_chain.generate(qs)\n",
    "# res\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e59eecd",
   "metadata": {},
   "source": [
    "It is a LLM, so we can try feeding in all questions at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd82e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(\n",
    "    template=multi_template,\n",
    "    input_variables=[\"questions\"]\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=flan_t5\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
    "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
    "    \"Who was the 12th person on the moon?\" +\n",
    "    \"How many eyes does a blade of grass have?\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ad1151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(llm_chain.run(qs_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7812c689",
   "metadata": {},
   "source": [
    "## Using OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e2057b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "189ff6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_TYPE\"] = openai.api_type = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = openai.api_version = \"2022-12-01\" #\"2023-03-15-preview\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = openai.api_base =  \"https://idocopenaigpt.openai.azure.com/\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key = \"95776649ac7a4b048c834003fd315264\"#os.getenv(\"AZUREOPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ef7f627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ramshankar\\anaconda3\\Lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:161: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://idocopenaigpt.openai.azure.com/ to https://idocopenaigpt.openai.azure.com/openai.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ramshankar\\anaconda3\\Lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:168: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ramshankar\\anaconda3\\Lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:176: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://idocopenaigpt.openai.azure.com/ to https://idocopenaigpt.openai.azure.com/openai.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"chatllm16k\",\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    "    model_name=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0854617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build prompt template for simple question-answering\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef1dfcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Green Bay Packers won the Super Bowl in the 2010 season.\n"
     ]
    }
   ],
   "source": [
    "question = \"Which NFL team won the Super Bowl in the 2010 season?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a07e09e",
   "metadata": {},
   "source": [
    "The same works again for multiple questions using generate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07c96735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text='The Green Bay Packers won the Super Bowl in the 2010 season.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='The Green Bay Packers won the Super Bowl in the 2010 season.'))], [ChatGeneration(text='You would be approximately 193 centimeters tall.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='You would be approximately 193 centimeters tall.'))], [ChatGeneration(text='The 12th person to walk on the moon was Eugene Cernan.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='The 12th person to walk on the moon was Eugene Cernan.'))], [ChatGeneration(text='A blade of grass does not have any eyes as it is a plant and does not possess visual organs.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='A blade of grass does not have any eyes as it is a plant and does not possess visual organs.'))]], llm_output={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 101, 'total_tokens': 163}, 'model_name': 'gpt-3.5-turbo'}, run=[RunInfo(run_id=UUID('b4f66bff-429a-44c3-bb00-88a5c4af3f1a')), RunInfo(run_id=UUID('b2945fd9-d6de-47a1-a767-341e08b55ebc')), RunInfo(run_id=UUID('2f142741-a1ba-4b77-88b8-232c08f645ab')), RunInfo(run_id=UUID('00584378-ba20-45d4-b1b3-32fcbd472f7d'))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs = [\n",
    "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
    "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
    "    {'question': \"Who was the 12th person on the moon?\"},\n",
    "    {'question': \"How many eyes does a blade of grass have?\"}\n",
    "]\n",
    "llm_chain.generate(qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec38956",
   "metadata": {},
   "source": [
    "Note that the below format doesn't feed the questions in iteratively but instead all in one chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5dce1d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Green Bay Packers won the Super Bowl in the 2010 season. \n",
      "\n",
      "If you are 6 ft 4 inches tall, you are approximately 193 centimeters tall. \n",
      "\n",
      "Eugene Cernan was the 12th person on the moon. \n",
      "\n",
      "A blade of grass does not have eyes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "qs = [\n",
    "    \"Which NFL team won the Super Bowl in the 2010 season?\",\n",
    "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\",\n",
    "    \"Who was the 12th person on the moon?\",\n",
    "    \"How many eyes does a blade of grass have?\"\n",
    "]\n",
    "print(llm_chain.run(qs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee7c1e",
   "metadata": {},
   "source": [
    "#### Now we can try to answer all question in one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87550384",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(\n",
    "    template=multi_template,\n",
    "    input_variables=[\"questions\"]\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=llm\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c1e1f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Green Bay Packers won the Super Bowl in the 2010 season.\n",
      "If you are 6 ft 4 inches tall, you are approximately 193 cm tall.\n",
      "The 12th person on the moon was Eugene Cernan.\n",
      "A blade of grass does not have any eyes.\n"
     ]
    }
   ],
   "source": [
    "qs_str = (\n",
    "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
    "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
    "    \"Who was the 12th person on the moon?\" +\n",
    "    \"How many eyes does a blade of grass have?\"\n",
    ")\n",
    "\n",
    "print(llm_chain.run(qs_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4142679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
