{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a6d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install tiktoken matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ed3c80",
   "metadata": {},
   "source": [
    "## For implemantation see this notebook : <a href=\"https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/xx-langchain-chunking.ipynb\" target=\"_blank\">https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/xx-langchain-chunking.ipynb/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6939919c",
   "metadata": {},
   "source": [
    "https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/xx-langchain-chunking.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81fff1f",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345aedec",
   "metadata": {},
   "source": [
    "In this example, we will download the LangChain docs from  <a href=\"langchain.readthedocs.io/\" target=\"_blank\">langchain.readthedocs.io/</a> We get all .html files located on the site like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e3a1c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad76cbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: wget.py [options]\n",
      "\n",
      "wget.py: error: no such option: -A\n"
     ]
    }
   ],
   "source": [
    "# !python -m wget -A.html rtdocs https://langchain.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ca86d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ramshankar\\anaconda3\\lib\\site-packages (from requests) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cea67da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import requests\n",
    "\n",
    "# [requests.get(link).content.save(os.path.join(\"rtdocs\", link.split(\"/\")[-1])) for link in requests.get(\"https://langchain.readthedocs.io/en/latest/\").text.split() if link.endswith(\".html\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7c8f62",
   "metadata": {},
   "source": [
    "This downloads all `HTML` into the rtdocs directory. Now we can use `LangChain` itself to process these docs. We do this using the `ReadTheDocsLoader` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba868be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'rtdocs'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "url = \"https://api.python.langchain.com/en/latest/agents/langchain.agents.agent.Agent.html#langchain.agents.agent.Agent\"\n",
    "wget.download(url, 'rtdocs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc098c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m wget --recursive --no-parent --no-clobber --page-requisites --html-extension --convert-links --timestamping -P rtdocs https://langchain.readthedocs.io/en/latest/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f54e1fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import ReadTheDocsLoader\n",
    "\n",
    "loader = ReadTheDocsLoader('rtdocs')\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc2eab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c1318",
   "metadata": {},
   "source": [
    "We access the plaintext page content like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f4d15f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(docs[0].page_content)\n",
    "\n",
    "# print(docs[5].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a5a653",
   "metadata": {},
   "source": [
    "We can also find the source of each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c30a34ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs[5].metadata['source'].replace('rtdocs/', 'https://')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cb2115",
   "metadata": {},
   "source": [
    "Looks good, we need to also consider the length of each page with respect to the number of tokens that will reasonably fit within the window of the latest LLMs. We will use gpt-3.5-turbo as an example.\n",
    "\n",
    "To count the number of tokens that gpt-3.5-turbo will use for some text we need to initialize the tiktoken tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58bda12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0cb05e",
   "metadata": {},
   "source": [
    "\n",
    "Note that for the tokenizer we defined the encoder as `\"cl100k_base\"`. This is a specific tiktoken encoder which is used by `gpt-3.5-turbo`, as well as `gpt-4`, and `text-embedding-ada-002` which are models supported by OpenAI at the time of this writing. Other encoders may be available, but are used with models that are now deprecated by OpenAI.\n",
    "\n",
    "You can find more details in the Tiktoken `model.py` script, or using `tiktoken.encoding_for_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91be729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiktoken.encoding_for_model('gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f18de82",
   "metadata": {},
   "source": [
    "Using the tiktoken_len function, let's count and visualize the number of tokens across our webpages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f799916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_counts = [tiktoken_len(doc.page_content) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb172ef1",
   "metadata": {},
   "source": [
    "Let's see `min`, `average`, and `max` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f7accc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\"\"Min: {min(token_counts)}\n",
    "# Avg: {int(sum(token_counts) / len(token_counts))}\n",
    "# Max: {max(token_counts)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ab2992",
   "metadata": {},
   "source": [
    "Now visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51e56d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # set style and color palette for the plot\n",
    "# sns.set_style(\"whitegrid\")\n",
    "# sns.set_palette(\"muted\")\n",
    "\n",
    "# # create histogram\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.histplot(token_counts, kde=False, bins=50)\n",
    "\n",
    "# # customize the plot info\n",
    "# plt.title(\"Token Counts Histogram\")\n",
    "# plt.xlabel(\"Token Count\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a0651b",
   "metadata": {},
   "source": [
    "The vast majority of pages seem to contain a lower number of tokens. But our limits for the number of tokens to add to each chunk is actually smaller than some of the smaller pages. But, how do we decide what this number should be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23a0fc",
   "metadata": {},
   "source": [
    "### Chunking the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d3d2cf",
   "metadata": {},
   "source": [
    "At the time of writing, `gpt-3.5-turbo` supports a context window of 4096 tokens â€” that means that input tokens + generated ( / completion) output tokens, cannot total more than 4096 without hitting an error.\n",
    "\n",
    "So we 100% need to keep below this. If we assume a very safe margin of ~2000 tokens for the input prompt into `gpt-3.5-turbo`, leaving ~2000 tokens for conversation history and completion.\n",
    "\n",
    "With this ~2000 token limit we may want to include five snippets of relevant information, meaning each snippet can be no more than 400 token long.\n",
    "\n",
    "To create these snippets we use the `RecursiveCharacterTextSplitter` from LangChain. To measure the length of snippets we also need a length function. This is a function that consumes text, counts the number of tokens within the text (after tokenization using the `gpt-3.5-turbo` tokenizer), and returns that number. We define it like so:\n",
    "\n",
    "With the length function defined we can initialize our `RecursiveCharacterTextSplitter` object like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8c5dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=400,\n",
    "#     chunk_overlap=20,  # number of tokens overlap between chunks\n",
    "#     length_function=tiktoken_len,\n",
    "#     separators=['\\n\\n', '\\n', ' ', '']\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c15f00a",
   "metadata": {},
   "source": [
    "Then we split the text for a document like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5d1da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks = text_splitter.split_text(docs[5].page_content)\n",
    "# len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c4939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks = text_splitter.split_text(docs[5].page_content)\n",
    "# len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31955cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiktoken_len(chunks[0]), tiktoken_len(chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75a12989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'abc-0',\n",
       "  'text': 'some important document text',\n",
       "  'source': 'https://langchain.readthedocs.io/en/latest/glossary.html'},\n",
       " {'id': 'abc-1',\n",
       "  'text': 'the next chunk of important document text',\n",
       "  'source': 'https://langchain.readthedocs.io/en/latest/glossary.html'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For docs[5] we created 2 chunks of token length 346 and 247.\n",
    "\n",
    "# This is for a single document, we need to do this over all of our documents. While we iterate through the docs to create these chunks we will reformat them into a format that looks like:\n",
    "\n",
    "[\n",
    "    {\n",
    "        \"id\": \"abc-0\",\n",
    "        \"text\": \"some important document text\",\n",
    "        \"source\": \"https://langchain.readthedocs.io/en/latest/glossary.html\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"abc-1\",\n",
    "        \"text\": \"the next chunk of important document text\",\n",
    "        \"source\": \"https://langchain.readthedocs.io/en/latest/glossary.html\"\n",
    "    }\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "# The \"id\" will be created based on the URL of the text + it's chunk number.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f850561",
   "metadata": {},
   "source": [
    "The \"id\" will be created based on the URL of the text + it's chunk number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd5753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hashlib\n",
    "# m = hashlib.md5()  # this will convert URL into unique ID\n",
    "\n",
    "# url = docs[5].metadata['source'].replace('rtdocs/', 'https://')\n",
    "# print(url)\n",
    "\n",
    "# # convert URL to unique ID\n",
    "# m.update(url.encode('utf-8'))\n",
    "# uid = m.hexdigest()[:12]\n",
    "# print(uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1640530c",
   "metadata": {},
   "source": [
    "Then use the uid alongside chunk number and actual url to create the format needed:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f1feead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [\n",
    "#     {\n",
    "#         'id': f'{uid}-{i}',\n",
    "#         'text': chunk,\n",
    "#         'source': url\n",
    "#     } for i, chunk in enumerate(chunks)\n",
    "# ]\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8835d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we repeat the same logic across our full dataset:\n",
    "\n",
    "\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# documents = []\n",
    "\n",
    "# for doc in tqdm(docs):\n",
    "#     url = doc.metadata['source'].replace('rtdocs/', 'https://')\n",
    "#     m.update(url.encode('utf-8'))\n",
    "#     uid = m.hexdigest()[:12]\n",
    "#     chunks = text_splitter.split_text(doc.page_content)\n",
    "#     for i, chunk in enumerate(chunks):\n",
    "#         documents.append({\n",
    "#             'id': f'{uid}-{i}',\n",
    "#             'text': chunk,\n",
    "#             'source': url\n",
    "#         })\n",
    "\n",
    "# len(documents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0d4205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're now left with 2201 documents. We can save them to a JSON lines (.jsonl) file like so:\n",
    "\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open('train.jsonl', 'w') as f:\n",
    "#     for doc in documents:\n",
    "#         f.write(json.dumps(doc) + '\\n')\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b6f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the data from file we'd write:\n",
    "\n",
    "# documents = []\n",
    "\n",
    "# with open('train.jsonl', 'r') as f:\n",
    "#     for line in f:\n",
    "#         documents.append(json.loads(line))\n",
    "\n",
    "# len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5411797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4142708b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
