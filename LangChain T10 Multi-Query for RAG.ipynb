{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a401d435",
   "metadata": {},
   "source": [
    "# LangChain Multi-Query for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa48b59",
   "metadata": {},
   "source": [
    "## For implemantation see this notebook : <a href=\"https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/10-langchain-multi-query.ipynb\" target=\"_blank\">https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/10-langchain-multi-query.ipynb</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU \\\n",
    "#   pinecone-client==2.2.4 \\\n",
    "#   langchain==0.0.321 \\\n",
    "#   datasets==2.14.6 \\\n",
    "#   openai==0.28.1 \\\n",
    "#   tiktoken==0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54e5ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# data = load_dataset(\"jamescalam/ai-arxiv-chunked\", split=\"train\")\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c62aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.docstore.document import Document\n",
    "\n",
    "# docs = []\n",
    "\n",
    "# for row in data:\n",
    "#     doc = Document(\n",
    "#         page_content=row[\"chunk\"],\n",
    "#         metadata={\n",
    "#             \"title\": row[\"title\"],\n",
    "#             \"source\": row[\"source\"],\n",
    "#             \"id\": row[\"id\"],\n",
    "#             \"chunk-id\": row[\"chunk-id\"],\n",
    "#             \"text\": row[\"chunk\"]\n",
    "#         }\n",
    "#     )\n",
    "#     docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8eb89",
   "metadata": {},
   "source": [
    "## Embedding and Vector DB Setup\n",
    "\n",
    "Initialize our embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d61bec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# model_name = \"text-embedding-ada-002\"\n",
    "\n",
    "# # get openai api key from platform.openai.com\n",
    "# OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or getpass(\"OpenAI API Key: \")\n",
    "\n",
    "# embed = OpenAIEmbeddings(\n",
    "#     model=model_name, openai_api_key=OPENAI_API_KEY, disallowed_special=()\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b9a9e6",
   "metadata": {},
   "source": [
    "Create our Pinecone index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1beebb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "import time\n",
    "\n",
    "# index_name = \"langchain-multi-query-demo\"\n",
    "\n",
    "# # find API key in console at app.pinecone.io\n",
    "# YOUR_API_KEY = os.getenv('PINECONE_API_KEY') or getpass(\"Pinecone API Key: \")\n",
    "# # find ENV (cloud region) next to API key in console\n",
    "# YOUR_ENV = os.getenv('PINECONE_ENVIRONMENT') or input(\"Pinecone Env: \")\n",
    "\n",
    "# pinecone.init(\n",
    "#     api_key=YOUR_API_KEY,\n",
    "#     environment=YOUR_ENV\n",
    "# )\n",
    "\n",
    "# if index_name not in pinecone.list_indexes():\n",
    "#     # we create a new index\n",
    "#     pinecone.create_index(\n",
    "#         name=index_name,\n",
    "#         metric='cosine',\n",
    "#         dimension=1536  # 1536 dim of text-embedding-ada-002\n",
    "#     )\n",
    "#     # wait for index to be initialized\n",
    "#     while not pinecone.describe_index(index_name).status[\"ready\"]:\n",
    "#         time.sleep(1)\n",
    "\n",
    "# # now connect to index\n",
    "# index = pinecone.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4e2383",
   "metadata": {},
   "source": [
    "Populate our index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9339981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33283417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to speed things up to follow along\n",
    "#docs = docs[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84fea399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "# batch_size = 100\n",
    "\n",
    "# for i in tqdm(range(0, len(docs), batch_size)):\n",
    "#     i_end = min(len(docs), i+batch_size)\n",
    "#     docs_batch = docs[i:i_end]\n",
    "#     # get IDs\n",
    "#     ids = [f\"{doc.metadata['id']}-{doc.metadata['chunk-id']}\" for doc in docs_batch]\n",
    "#     # get text and embed\n",
    "#     texts = [d.page_content for d in docs_batch]\n",
    "#     embeds = embed.embed_documents(texts=texts)\n",
    "#     # get metadata\n",
    "#     metadata = [d.metadata for d in docs_batch]\n",
    "#     to_upsert = zip(ids, embeds, metadata)\n",
    "#     index.upsert(vectors=to_upsert)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50f406c",
   "metadata": {},
   "source": [
    "## Multi-Query with LangChain\n",
    "\n",
    "Now we switch across to using our populated index as a vectorstore in Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e5d6dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.vectorstores import Pinecone\n",
    "\n",
    "# text_field = \"text\"\n",
    "\n",
    "# vectorstore = Pinecone(index, embed.embed_query, text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "746c1ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505b4b81",
   "metadata": {},
   "source": [
    "We initialize the `MultiQueryRetriever`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "796802ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# retriever = MultiQueryRetriever.from_llm(\n",
    "#     retriever=vectorstore.as_retriever(), llm=llm\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac803a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a59e72c",
   "metadata": {},
   "source": [
    "To query with our multi-query retriever we call the `get_relevant_documents method`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f75023c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"tell me about llama 2?\"\n",
    "\n",
    "# docs = retriever.get_relevant_documents(query=question)\n",
    "# len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45d552a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb50cd6b",
   "metadata": {},
   "source": [
    "###  Adding the Generation in RAG\n",
    "\n",
    "So far we've built a multi-query powered Retrieval Augmentation chain. Now, we need to add Generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b4772bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "from getpass import getpass\n",
    "from langchain import OpenAI\n",
    "from langchain import PromptTemplate \n",
    "\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "import tiktoken\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e3c4b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_TYPE\"] = openai.api_type = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = openai.api_version = \"2022-12-01\" #\"2023-03-15-preview\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = openai.api_base =  \"https://idocopenaigpt.openai.azure.com/\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key = \"95776649ac7a4b048c834003fd315264\"#os.getenv(\"AZUREOPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff826c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ramshankar\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.azure_openai.AzureChatOpenAI` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. Use langchain_openai.AzureChatOpenAI instead.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\Ramshankar\\anaconda3\\Lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:165: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://idocopenaigpt.openai.azure.com/ to https://idocopenaigpt.openai.azure.com/openai.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ramshankar\\anaconda3\\Lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:172: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ramshankar\\anaconda3\\Lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:180: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://idocopenaigpt.openai.azure.com/ to https://idocopenaigpt.openai.azure.com/openai.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"chatllm16k\",\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e16fceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"contexts\"],\n",
    "    template=\"\"\"You are a helpful assistant who answers user queries using the\n",
    "    contexts provided. If the question cannot be answered using the information\n",
    "    provided say \"I don't know\".\n",
    "\n",
    "    Contexts:\n",
    "    {contexts}\n",
    "\n",
    "    Question: {query}\"\"\",\n",
    ")\n",
    "\n",
    "# Chain\n",
    "qa_chain = LLMChain(llm=llm, prompt=QA_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "022b911d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ramshankar\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I\\'m sorry, but without any information about \"llama 2,\" I don\\'t have enough context to provide an answer. Could you please provide more details or clarify your question?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = qa_chain(\n",
    "    inputs={\n",
    "        \"query\": question,\n",
    "        \"contexts\": \"\\n---\\n\".join([d.page_content for d in docs])\n",
    "    }\n",
    ")\n",
    "out[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae08253",
   "metadata": {},
   "source": [
    "### Chaining Everything with a SequentialChain\n",
    "\n",
    "We can pull together the logic above into a function or set of methods, whatever is prefered — however if we'd like to use LangChain's approach to this we must \"chain\" together multiple chains. The first retrieval component is (1) not a chain per se, and (2) requires processing of the output. To do that, and fit with LangChain's \"chaining chains\" approach, we setup the retrieval component within a `TransformChain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1b88c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import TransformChain\n",
    "\n",
    "def retrieval_transform(inputs: dict) -> dict:\n",
    "    docs = retriever.get_relevant_documents(query=inputs[\"question\"])\n",
    "    docs = [d.page_content for d in docs]\n",
    "    docs_dict = {\n",
    "        \"query\": inputs[\"question\"],\n",
    "        \"contexts\": \"\\n---\\n\".join(docs)\n",
    "    }\n",
    "    return docs_dict\n",
    "\n",
    "retrieval_chain = TransformChain(\n",
    "    input_variables=[\"question\"],\n",
    "    output_variables=[\"query\", \"contexts\"],\n",
    "    transform=retrieval_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd55c28b",
   "metadata": {},
   "source": [
    "Now we chain this with our generation step using the `SequentialChain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a484cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "rag_chain = SequentialChain(\n",
    "    chains=[retrieval_chain, qa_chain],\n",
    "    input_variables=[\"question\"],  # we need to name differently to output \"query\"\n",
    "    output_variables=[\"query\", \"contexts\", \"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc153e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we perform the full RAG pipeline:\n",
    "\n",
    "\n",
    "# out = rag_chain({\"question\": question})\n",
    "# out[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03489d63",
   "metadata": {},
   "source": [
    "## Custom Multiquery\n",
    "\n",
    "We'll try this with two prompts, both encourage more variety in search queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2df495",
   "metadata": {},
   "source": [
    "**Prompt A**\n",
    "\n",
    "    Your task is to generate 3 different search queries that aim to\n",
    "    answer the user question from multiple perspectives.\n",
    "    Each query MUST tackle the question from a different viewpoint,\n",
    "    we want to get a variety of RELEVANT search results.\n",
    "    Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea9a2da",
   "metadata": {},
   "source": [
    "__Prompt B__\n",
    "\n",
    "    Your task is to generate 3 different search queries that aim to\n",
    "    answer the user question from multiple perspectives. The user questions\n",
    "    are focused on Large Language Models, Machine Learning, and related\n",
    "    disciplines.\n",
    "    Each query MUST tackle the question from a different viewpoint, we\n",
    "    want to get a variety of RELEVANT search results.\n",
    "    Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2161b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.chains import LLMChain\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "# Output parser will split the LLM result into a list of queries\n",
    "class LineList(BaseModel):\n",
    "    # \"lines\" is the key (attribute name) of the parsed output\n",
    "    lines: List[str] = Field(description=\"Lines of text\")\n",
    "\n",
    "\n",
    "class LineListOutputParser(PydanticOutputParser):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "\n",
    "    def parse(self, text: str) -> LineList:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return LineList(lines=lines)\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab7b806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Your task is to generate 3 different search queries that aim to\n",
    "answer the user question from multiple perspectives. The user questions\n",
    "are focused on Large Language Models, Machine Learning, and related\n",
    "disciplines.\n",
    "Each query MUST tackle the question from a different viewpoint, we\n",
    "want to get a variety of RELEVANT search results.\n",
    "Provide these alternative questions separated by newlines.\n",
    "Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=template,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f3d85f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1cf0ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run\n",
    "# retriever = MultiQueryRetriever(\n",
    "#     retriever=vectorstore.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\n",
    "# )  # \"lines\" is the key (attribute name) of the parsed output\n",
    "\n",
    "# # Results\n",
    "# docs = retriever.get_relevant_documents(\n",
    "#     query=question\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29e0f585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0550d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieval_chain = TransformChain(\n",
    "#     input_variables=[\"question\"],\n",
    "#     output_variables=[\"query\", \"contexts\"],\n",
    "#     transform=retrieval_transform\n",
    "# )\n",
    "\n",
    "# rag_chain = SequentialChain(\n",
    "#     chains=[retrieval_chain, qa_chain],\n",
    "#     input_variables=[\"question\"],  # we need to name differently to output \"query\"\n",
    "#     output_variables=[\"query\", \"contexts\", \"text\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdee26b",
   "metadata": {},
   "source": [
    "And asking again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c947dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = rag_chain({\"question\": question})\n",
    "# out[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecd4c49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pinecone.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b7638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
